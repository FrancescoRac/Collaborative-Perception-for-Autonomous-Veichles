# Collaborative-Perception-for-Autonomous-Veichles

Download the data for the simulation here: https://uncloud.univ-nantes.fr/index.php/s/oiMzTpqiBGniZng

- Unzip the folder;
- In the file *utils* make sure the *root_path* is set to the root of the scenario1 folder.

### Description

This lab focuses on utilizing Intelligent Road-Side Unit (IRSU) data to enhance vehicle perception and environment mapping. It provides hands-on experience with LiDAR data, helping participants learn how to effectively visualize and interpret it for autonomous vehicle applications.

## Steps

### Task 1

Visualizing LiDAR Data from IRSU:
- Process and visualize point cloud data from a 32-channel LiDAR.

![image](https://github.com/user-attachments/assets/cd1b496e-0b1d-429e-8a58-04197dc1a019)



### Task 2
Enhancing Vehicle Perception with IRSU Data:
- Learn how IRSU data can enhance the vehicle's field of view and perception range.

![image](https://github.com/user-attachments/assets/16cbc10b-edbf-43d1-85ba-8a670e8360ad)



### Task 3
Building an Occupancy Grid:
- Construct a Bird's Eye View (BEV) occupancy grid of the environment.
- Understand the significance of occupancy grids in autonomous vehicle navigation.

![image](https://github.com/user-attachments/assets/152d749f-8dcc-4027-b540-e4a2709c22d3)

![image](https://github.com/user-attachments/assets/02087385-4a83-4857-b2ae-6029777fef8e)



### Task 4
Segment Objects in the image:
- Segement in the Environment the objects based on the type, then build the BEV image showing only the objetcs chosen.

![image](https://github.com/user-attachments/assets/ffd5b55b-8b6c-418e-94f8-9d6b08792cf1)

![image](https://github.com/user-attachments/assets/3356b118-27dc-46c2-81bf-d45255bea4c7)

![image](https://github.com/user-attachments/assets/0f841532-8488-4399-9f00-0f53a671ad8b)

![image](https://github.com/user-attachments/assets/345dabb6-876d-4707-8a88-0177cb76285e)




